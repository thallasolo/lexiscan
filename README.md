ğŸ—“ï¸ WEEK 1 â€” OCR Pipeline & Data Preparation
ğŸ¯ Goal
Prepare high-quality text data from raw legal PDFs for downstream NLP model training.
ğŸ”§ What was done
1ï¸âƒ£ Project setup

Created a clean folder structure
Set up Python environment
Initialized GitHub repository

data/raw_pdfs/      â†’ input PDF contracts  
data/ocr_text/      â†’ extracted OCR text  
ocr/                â†’ OCR pipeline code  

2ï¸âƒ£ PDF â†’ Image conversion
Used Poppler with pdf2image
Converted each PDF page into high-resolution images (dpi=300)
images = convert_from_bytes(pdf_bytes, dpi=300)

3ï¸âƒ£ OCR using Tesseract
Extracted text from images
Handled scanned + image-only contracts
Stored text as .txt files for review

4ï¸âƒ£ OCR Quality Check (Manual)
Reviewed extracted text to verify:
Dates are readable
Party names are intact
Amounts are visible

ğŸ“Œ Bad OCR â†’ Fix preprocessing before annotation
5ï¸âƒ£ Output of Week 1
âœ” Clean text data
âœ” OCR pipeline ready
âœ” Ready for annotation

âœ… Validation
OCR text manually inspected
Notes documented in annotation_notes.md

ğŸ—“ï¸ WEEK 2 â€” Named Entity Recognition (NER)
ğŸ¯ Goal
Train a custom NLP model to understand legal language and extract entities.
ğŸ”§ What was done
1ï¸âƒ£ Annotation creation
Created data/annotated/train_annotations.json
Annotated 30â€“50 samples
Used BIO-style entity spans

Entities:
PARTY
DATE
AMOUNT
JURISDICTION

Example:
{
  "text": "Agreement dated February 1, 2018 between Zynga Inc. and WPT Enterprises.",
  "entities": [
    [17, 33, "DATE"],
    [42, 51, "PARTY"],
    [56, 71, "PARTY"]
  ]
}

2ï¸âƒ£ Convert annotations to SpaCy format
python ner/convert_to_spacy.py

Output:
ner/train.spacy
3ï¸âƒ£ Train custom SpaCy NER model
python ner/train.py

Loss decreased across epochs
Model saved as:
ner/lexiscan_ner_model/

4ï¸âƒ£ Evaluation
Used F1-score to evaluate performance
Early-stage model showed learning signal

âœ… Validation
âœ” Model successfully predicts dates and parties
âœ” Ready for inference + improvement loop

ğŸ—“ï¸ WEEK 3 â€” Rule-Based Precision Layer
ğŸ¯ Goal
Improve precision and handle legal edge cases that ML alone cannot.

ğŸ”§ What was done
1ï¸âƒ£ Post-processing rules added
Handled problems like:
(i), (30), (the â€œ false positives

Section headers being misclassified as parties
2ï¸âƒ£ Party cleanup logic
Removed noisy tokens
Ensured realistic company names
Allowed OCR-broken company names
if len(p.split()) >= 3:
    keep

3ï¸âƒ£ Normalization rules
Dates â†’ ISO-8601 format
Amounts â†’ numeric values

âœ… Validation
âœ” Garbage entities reduced by ~80â€“90%
âœ” Output became legally meaningful

ğŸ—“ï¸ WEEK 4 â€” API, Docker & End-to-End Pipeline
ğŸ¯ Goal
Deploy the entire system as a production-ready service.

ğŸ”§ What was done
1ï¸âƒ£ FastAPI backend
Created /extract endpoint:
Accepts PDF upload
Returns structured JSON

2ï¸âƒ£ End-to-End flow
PDF â†’ Image â†’ OCR â†’ NER â†’ Rules â†’ JSON

3ï¸âƒ£ Dockerization
Installed OCR dependencies inside Docker
Ensured OS-independent deployment
docker build -t lexiscan-auto .
docker run -p 8000:8000 lexiscan-auto

4ï¸âƒ£ Browser support
Used FastAPI Swagger UI
http://localhost:8000/docs
Upload PDF â†’ Click Execute â†’ Get JSON

5ï¸âƒ£ Automation (Optional)
Folder watcher auto-processes new PDFs
Simulates production ingestion pipeline

âœ… Validation
âœ” API runs in Docker
âœ” Browser & curl both work
âœ” Unseen PDFs processed successfully
